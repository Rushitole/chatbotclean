# ğŸ¤– Rushi's AI Bot (Django + Ollama)

A simple AI chatbot built using Django backend and Ollama local LLM.

---

## ğŸ“Œ Tech Stack Used

- Python 3.10+
- Django
- Requests
- Ollama (Local LLM server)
- TinyLlama / Gemma Model

---

## ğŸ–¥ System Requirements

- 8GB RAM minimum
- Windows / Mac / Linux
- Python installed
- Ollama installed

---

## âš™ï¸ Step 1: Clone Project

bash
git clone <your-repo-url>
cd project-folder


---

## âš™ï¸ Step 2: Create Virtual Environment

bash
python -m venv venv


Activate:

Windows:
bash
venv\Scripts\activate


Mac/Linux:
bash
source venv/bin/activate


---

## âš™ï¸ Step 3: Install Requirements

bash
pip install -r requirements.txt


---

## âš™ï¸ Step 4: Install Ollama

Download from:
https://ollama.com

After install, verify:

bash
ollama --version


---

## âš™ï¸ Step 5: Pull Model

Recommended lightweight model:

bash
ollama pull tinyllama


OR

bash
ollama pull gemma:2b


Check installed models:

bash
ollama list


---

## âš™ï¸ Step 6: Start Ollama Server

bash
ollama serve


(Default runs on http://127.0.0.1:11434)

âš ï¸ If port already in use, Ollama is already running.

---

## âš™ï¸ Step 7: Run Django Server

bash
python manage.py migrate
python manage.py runserver


Open browser:


http://127.0.0.1:8000/chat/


---

## ğŸ”Œ API Used

Ollama API Endpoint:


POST http://127.0.0.1:11434/api/generate


Request JSON:

json
{
  "model": "tinyllama:latest",
  "prompt": "Hello",
  "stream": false
}


---

## ğŸ§  Features

- Dark UI
- Instant reply feel
- Online status indicator
- Local LLM (No OpenAI key required)
- Fully offline AI chatbot

---

## ğŸš€ Performance Notes

- First reply may take 8â€“15 seconds (model loading)
- Subsequent replies are faster (2â€“4 seconds)
- Works best on 8GB+ RAM systems

---

## ğŸ›  Dependencies

requirements.txt:


Django>=4.2,<6.0
requests>=2.31.0


Install with:

bash
pip install -r requirements.txt


---

## ğŸ›‘ Common Issues

### Port 11434 already in use
Ollama already running. Do NOT start again.

### Slow first reply
Normal behavior (model loading).

### No reply
Check:
- Ollama running?
- Correct model name in views.py?
- Correct API URL?

---

## ğŸ‘¨â€ğŸ’» Developed By

Rushi